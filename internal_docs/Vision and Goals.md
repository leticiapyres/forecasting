## Vision 

Establish an industry leading framework that allows benchmarking of various forecasting algorithms and frameworks in a practical way on real cloud based architectures. This will allow potential or customers to discover the best approach that suites their needs from cost, time and quality perspective.

The benchmarking framework is designed to facilitate community participation and contribution through the development of benchmark implementations against a practical set of forecasting problems and datasets. These implementations will be measured by means of standard metrics of accuracy, cost and training time.

The benchmarking framework will provide a simple access, discovery, comparison, maintenance and contributions. 

Note: The TSPerf vision is aligned with the [MLPerf](https://mlperf.org/) vision and designed to be merged with it after an internal implementation of it. 

 
 
## Goals:  

Short Term (Internal Only): 
* Allow fair benchmarking /comparison of various forecasting frameworks/algorithms' performance on a given problem/dataset on multiple environments with the ability to reproduce it.
* To become the central Microsoft repository for forecasting benchmarks
 
Future: 
Integrate TSPerf as a new track in [MLPerf](https://mlperf.org/).

## Documentation Structure 
This document serves as the master document and includes the generic concepts and scope of the forecasting framework.
In addition to it there will be a specific benchmark submission guidelines documents that are part of each the benchmarks that are included in the framework. These will include the problem description, implementation and submission instructions. 
